Конспект доклада А. Полова с DevOpsConf 2023 - Istio в разрезе: что умеет и не умеет самый популярный Service Mesh.
Так же к конспекту есть презентация на сайте speakerdeck.com. Поиск производится по аналогичному названию доклада.

Service Mesh - технология, с помощью которой можно взять контроль над всем TCP-шным траффиком в вашем проекте, реализовать те или иные паттерны по управлению сетевым траффиком.
Все это найстраивается с помощью декларативного языка API Kubernetes.

------- Как работает Istio. Базовые знания. -------
У нас есть типовой проект в Кубере. Каждый компонент живет в ПОДе.
После внедрения Istio в кластере Кубера появлется контроллер (istiod) - ControlPlane.
К каждому из наших ПОДов, в нагрузку, выдадут по сайдкару (istio-proxy).
Весь массив сайдкаров, в кластере Кубера, называется DataPlane.
Сайдкары перехватывают прикладные запросы (запросы к приложениям) и обрабатывают их, согласно настройкам, которые делаем через API в Кубе.
ControlPlane выгребает эти настройки, обрабатывает и рассылает сайдкарам инструкции как им быть, как им работать.

------- Что находится внутри обычного ПОДа Кубера. -------
1. Контейнер с нашим приложением.
2. Сайдкар Istio-proxy.

------- Что находится внутри сайдкара Istio-proxy. -------
1. Envoy (прокся), которая принимает TCP/HTTP-гные запросы, обрабатывает их и отдает наружу.
Envoy дает нам интерфейс для управления принятием решения данного запроса - envoyAPI.
2. Istio-agent (демон, который создает Istio). Держит связь с ControlPlane, выгребает из ControlPlane всю информацию из кластера и переваривает ее в понятный Envoy'ю язык. И таким образом Envoy получает информацию о состоянии кластера.

------- Что из себя представляет Envoy. Базовые понятия. -------
Все начинается с примитива, который называется Listener. Listener - это целый конвейер по обработке входящего запроса.
Он содержит фильтры по обработке входящего траффика. Этот конвейер может состоять из сколь угодно большого объема этих самых фильтров. И они могут идти в любом порядке.
Но, как правильно, вся цепочка фильтров, в Listener'е, заканчивается правилом Route. В конце цепочки принимается ррешение куда полетит запрос.
Решение основывается на основе Cluster (в рамках Envoy). В Cluster находится информация и сервисе, куда полетит запрос.
Там есть:
- список endpoints (ip:port) сервиса;
- есть правила балансировки между этими айпишниками (loadBalance);
- так же в Cluster'е есть серты для установки шифрованного исходящего соединения;
- так же в Cluster'е содержатся правила для калькуляции метрик;
------- Конец пути запроса в Envoy. -------

------- Как сайдкар Istio-proxy перехватывает входящие запросы. -------
В каждом ПОДе(!), под управлением Istio, настроен DNAT, который перенаправляет траффик, на специальный Listener, который ему настроил сам Istio, на порте 15001 (для исходящего траффика) и на порте 15006 (для входящего траффика).
-----------------------------------------------------------------------

В Кубе у каждого компонента работающего приложения есть Сервис.
У каждого ПОДа есть свой АйПишник.
Эту инфу Истио выгребает из API Куба, переваривает и рассылает по Энвоям, через Сайдкары.

На каждый Сервис Кубера в Envoy'е создается Cluster, в котором содержатся:
- список эндпоинтов, АйПишники ПОДов, номер порта из Сервиса и все это находится в эндпоинтах нашего Cluster'а.

По итогу, у нас появляется список Cluster'ов, между которыми мы можем произвести маршрутизацию. Берем запрос, у запроса есть параметры, на основании которых мы принимаем решение, куда лететь дальше запросу.
Когда мы перехватили запрос, у нас есть:
- TCP IP:port (куда летел запрос);
- HTTP Host (если HTTP-шный траффик, то есть заголовок host);
- TLS SNI (если траффик зашифрован на стороне приложения, то есть SNI);

Для разного типа запроса (TCP/HTTP) создается свой отдельный Listener, чтобы различать типы(!) запросов.
ВАЖНО(!)
Listener'ы, которые создаются для каждого типа запроса НЕ ЯВЛЯЮТСЯ Listener'ом, которые перехватывают запросы на уровне Сайдакара. На уровне Сайдкара всегда(!) один(!) Listener на вход(!) и один Listener на выход(!).
Эти Listener'ы выше на один уровень, чем те, о которых мы говорим сейчас. Listener на уровень выше перехватывают запрос и направляет его в Listener на уровень ниже.
Listener на уровень выше имеет флаг use_original_dest, который позволяет узнать у ядра оригинальный destination перед процедурой DNAT.
И на основе этих данный Энвой передает обработку запроса соответствующему Listener'у.            

DestinationRule отвечает за все нюансы установки исходящего соединения. Балансировка между эндпоинтами, настройка TCP-шных параметров, пассивный healthcheck.

------- Как запросы ходят в рамках взаимодействия в Истио -------
Исходные данные: Есть два ПОДа (Сервис А, Сервис Б).
Задача: Сервису А нужно дернуть Сервис Б.
Процесс:
1. Из сервиса А уходит запрос на GET http://back:8080.
2. Сервис А резолвит хост, получает АйПишник, получает порт и делает запрос в рамках Istio.
3. Envoy этот запрос перехватывает, парсит хост и порт, ищет соответствующий Listenet с соответствующим портом, отправляет запрос в этот Listener и обрабатвает запрос уже в нем.
4. В Listener'е:
4.1 - валидируется tls (handle tls);
4.2 - калькулируются метрики для экспорта, в дальнейшем, в Прометей (handle stats);
4.3 - решается вопрос об авторизации, можно или нельзя запросу вылететь наружу (authorization);
4.4 - производится маршрутизация в Cluster (route);
5. Выбирается Cluster с соответствующим заголовком (в данном случае back, взялся из хоста запроса);
6. В Cluster'е:
6.1 - производится балансировка между endpoint'ами, которые присутствуют в Cluster'е (loadBalance -> select endpoint);
6.2 - устанавливаем TPC-шный коннект (connect);
6.3 - пополняем статистику для пассивного healthcheck (outlier detection);
6.4 - выгребаем сертификаты, для установки шифрованного соединения исходящего (handle tls);
6.5 - калькулируем метрики и отправляем запрос из Listener'а наружу, в данном случае это будет прокся Istio-proxy, сайдкар, который принадлежит Сервису Б, куда, собственно, и уйдет сам запрос (handle stats).
------- Конец процесса -------
